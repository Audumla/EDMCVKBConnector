import pytest
from unittest.mock import patch, MagicMock
from pathlib import Path
import json
import os
import sys

# Add scripts to path so we can import them
sys.path.append(str(Path(__file__).resolve().parent.parent / "scripts" / "changelog"))

from changelog_utils import (
    normalize_llm_summary,
    run_llm_with_fallback,
    load_config,
)

@pytest.fixture
def mock_config():
    return {
        "common": {
            "timeout_seconds": 300,
            "fallback_order": ["claude-cli", "codex", "gemini", "copilot"]
        },
        "claude_cli": {"model": "haiku"},
        "codex": {"model": "5.3"},
        "gemini": {"model": ""},
        "copilot": {"model": "gpt-4.1"}
    }

def test_normalize_llm_summary_basic():
    summary = "### Overview\nSome intro.\n\n### Bug Fixes\n- Fix A\n\n### New Features\n- Feature B"
    normalized = normalize_llm_summary(summary)
    assert "### Overview" in normalized
    assert "### Bug Fixes" in normalized
    assert "### New Features" in normalized
    assert "Some intro." in normalized

def test_normalize_llm_summary_deduplication():
    summary = "### Overview\nSummary.\n\n### Bug Fixes\n- Fix 1\n\n### Bug Fixes\n- Fix 2"
    normalized = normalize_llm_summary(summary)
    assert normalized.count("### Bug Fixes") == 1
    assert "- Fix 1" in normalized
    assert "- Fix 2" in normalized

def test_normalize_llm_summary_ensures_overview():
    summary = "### Bug Fixes\n- Fix A"
    normalized = normalize_llm_summary(summary)
    assert "### Overview" in normalized
    assert "Summary of changes in this release." in normalized

def test_normalize_llm_summary_strips_metadata():
    summary = "Here are the release notes:\n\n### Overview\nChanges.\n\nNote: This was generated by AI.\n---\nFooter."
    normalized = normalize_llm_summary(summary)
    assert "Here are the release notes:" not in normalized
    assert "Note: This was generated by AI." not in normalized
    assert "Footer." not in normalized
    assert "### Overview" in normalized

@patch("changelog_utils.call_claude_cli")
@patch("changelog_utils.call_codex_cli")
@patch("changelog_utils.call_gemini_cli")
@patch("changelog_utils.call_copilot_cli")
def test_run_llm_with_fallback_success_first(mock_copilot, mock_gemini, mock_codex, mock_claude, mock_config):
    mock_claude.return_value = "### Overview\nSuccess"
    
    result, backend = run_llm_with_fallback("prompt", mock_config, preferred_backend="claude-cli")
    
    assert result is not None
    assert "Success" in result
    assert backend == "claude-cli"
    mock_claude.assert_called_once()
    mock_codex.assert_not_called()

@patch("changelog_utils.call_claude_cli")
@patch("changelog_utils.call_codex_cli")
@patch("changelog_utils.call_gemini_cli")
@patch("changelog_utils.call_copilot_cli")
def test_run_llm_with_fallback_sequental(mock_copilot, mock_gemini, mock_codex, mock_claude, mock_config):
    # Claude fails, Codex succeeds
    mock_claude.return_value = None
    mock_codex.return_value = "### Overview\nCodex Success"
    
    result, backend = run_llm_with_fallback("prompt", mock_config, preferred_backend="claude-cli")
    
    assert result is not None
    assert "Codex Success" in result
    assert backend == "codex"
    mock_claude.assert_called_once()
    mock_codex.assert_called_once()
    mock_gemini.assert_not_called()

@patch("changelog_utils.call_claude_cli")
@patch("changelog_utils.call_codex_cli")
@patch("changelog_utils.call_gemini_cli")
@patch("changelog_utils.call_copilot_cli")
def test_run_llm_with_fallback_all_fail(mock_copilot, mock_gemini, mock_codex, mock_claude, mock_config):
    mock_claude.return_value = None
    mock_codex.return_value = None
    mock_gemini.return_value = None
    mock_copilot.return_value = None
    
    result, backend = run_llm_with_fallback("prompt", mock_config)
    
    assert result is None
    assert backend == "none"
    assert mock_claude.called
    assert mock_codex.called
    assert mock_gemini.called
    assert mock_copilot.called

# ---------------------------------------------------------------------------
# Live Agent Tests (Skipped by default, run with --run-live-agents)
# ---------------------------------------------------------------------------

@pytest.mark.live_agent
@pytest.mark.parametrize("backend", ["claude-cli", "codex", "gemini", "copilot"])
def test_live_agent_output_format(backend):
    """
    Call the REAL agent and ensure the output is normalized and follows the template.
    This will consume real tokens/usage.
    """
    config = load_config()
    if not config:
        pytest.skip("No changelog-config.json found for live test.")
        
    sample_prompt = """
    # Summarize changelog entries for version 1.0.0
    
    Entries:
    [Bug Fix] Fixed a crash in the engine.
      - Resolved race condition in thread handler.
    [New Feature] Added a new landing gear toggle.
      - Users can now map a button to toggle gear state.
    
    ---
    Write a human-readable changelog summary that starts with ### Overview and groups changes.
    """
    
    from changelog_utils import run_llm_with_fallback
    
    # We use run_llm_with_fallback with preferred_backend to target the specific agent
    result, used_backend = run_llm_with_fallback(sample_prompt, {"common": config.get("common", {}), backend.replace("-", "_"): config.get(backend.replace("-", "_"), {})}, preferred_backend=backend)
    
    assert used_backend == backend, f"Expected to use {backend}, but used {used_backend}"
    assert result is not None, f"Agent {backend} returned no output"
    assert len(result) > 50, f"Agent {backend} output is too short"
    
    # Validation of requested format
    assert "### Overview" in result, f"Agent {backend} output missing ### Overview"
    # Normalization should have ensured it's at the top (after potential intro lines which normalization moves into Overview)
    assert result.startswith("### Overview"), f"Agent {backend} output should start with ### Overview after normalization"
    
    # Check for categories (at least one of these should likely be there if the LLM followed instructions)
    # Note: We can't be 100% sure of exact wording, but normalization ensures the ### format.
    assert "### " in result
    
    # Ensure no conversational filler or metadata
    assert "Here are" not in result[:20].lower()
    assert "Note:" not in result.lower()
    assert "---" not in result
